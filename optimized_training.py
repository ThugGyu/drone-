#!/usr/bin/env python3
"""
ğŸ“ ìµœì í™”ëœ ê°•í™”í•™ìŠµ ë“œë¡  í›ˆë ¨ ì‹œìŠ¤í…œ
- ë” ë¹ ë¥¸ í•™ìŠµ ì†ë„
- í–¥ìƒëœ ìˆ˜ë ´ì„±
- ë©”ëª¨ë¦¬ íš¨ìœ¨ì ì¸ í™˜ê²½
"""

import numpy as np
import torch
import torch.nn as nn
from stable_baselines3 import PPO
from stable_baselines3.common.vec_env import SubprocVecEnv, VecNormalize
from stable_baselines3.common.callbacks import BaseCallback
import gymnasium as gym
from gymnasium import spaces
import multiprocessing as mp
from typing import Dict, Any
import wandb  # ì‹¤í—˜ ì¶”ì ìš©

class OptimizedDroneEnv(gym.Env):
    """ìµœì í™”ëœ ë“œë¡  í™˜ê²½ - ë¹ ë¥¸ í•™ìŠµìš©"""
    
    def __init__(self, config: Dict[str, Any] = None):
        super().__init__()
        
        self.config = config or self.get_default_config()
        
        # ì‘ì€ ì•¡ì…˜ ìŠ¤í˜ì´ìŠ¤ë¡œ í•™ìŠµ íš¨ìœ¨ì„± í–¥ìƒ
        self.action_space = spaces.Box(
            low=-1.0, high=1.0, shape=(2,), dtype=np.float32  # [x, y] ì´ë™ë§Œ
        )
        
        # ì••ì¶•ëœ ê´€ì°° ìŠ¤í˜ì´ìŠ¤ (12ì°¨ì›ìœ¼ë¡œ ì¶•ì†Œ)
        self.observation_space = spaces.Box(
            low=-np.inf, high=np.inf, shape=(12,), dtype=np.float32
        )
        
        # ìµœì í™”ëœ ìº í¼ìŠ¤ ë°ì´í„° (numpy ë°°ì—´)
        self.buildings = np.array([
            [90, 70, 40, 30],   # Engineering [x, y, w, h]
            [190, 90, 35, 25],  # Library
            [40, 140, 30, 40],  # Dormitory
            [170, 40, 25, 25],  # Student Hall
            [240, 140, 30, 35], # Gymnasium
        ], dtype=np.float32)
        
        # ë‹¨ìˆœí™”ëœ ê²½ìœ ì  (5ê°œë¡œ ì¶•ì†Œ)
        self.waypoints = np.array([
            [30, 30],   # Start
            [100, 80],  # Checkpoint 1
            [150, 120], # Checkpoint 2
            [100, 120], # Checkpoint 3
            [30, 30],   # End
        ], dtype=np.float32)
        
        self.reset()
    
    @staticmethod
    def get_default_config():
        """ê¸°ë³¸ ì„¤ì •"""
        return {
            "max_steps": 500,
            "success_reward": 100.0,
            "collision_penalty": -50.0,
            "progress_reward_scale": 10.0,
            "step_penalty": -0.1,
            "boundary_penalty": -10.0
        }
    
    def reset(self, seed=None):
        """ë¹ ë¥¸ ë¦¬ì…‹"""
        if seed is not None:
            np.random.seed(seed)
        
        # ì´ˆê¸° ìƒíƒœ
        self.position = np.array([30.0, 30.0], dtype=np.float32)
        self.velocity = np.zeros(2, dtype=np.float32)
        self.current_waypoint = 1
        self.step_count = 0
        self.visited = [0]
        
        # ì„±ëŠ¥ ì¶”ì  ë³€ìˆ˜ë“¤
        self.last_distance = self._distance_to_target()
        self.total_reward = 0.0
        
        return self._get_observation(), {}
    
    def _get_observation(self):
        """ì••ì¶•ëœ 12ì°¨ì› ê´€ì°°ê°’"""
        target = self.waypoints[self.current_waypoint]
        to_target = target - self.position
        distance = np.linalg.norm(to_target)
        
        # 4ë°©í–¥ ê°„ë‹¨í•œ ì„¼ì„œ
        sensors = self._get_simple_sensors()
        
        obs = np.concatenate([
            self.position / 150.0,              # ì •ê·œí™”ëœ ìœ„ì¹˜ (2)
            to_target / 200.0,                  # ëª©í‘œ ë°©í–¥ (2)
            [distance / 200.0],                 # ëª©í‘œ ê±°ë¦¬ (1)
            [self.current_waypoint / len(self.waypoints)], # ì§„í–‰ë¥  (1)
            sensors,                            # 4ë°©í–¥ ì„¼ì„œ (4)
            self.velocity / 5.0,                # ì†ë„ (2)
        ])
        
        return obs.astype(np.float32)
    
    def _get_simple_sensors(self):
        """4ë°©í–¥ ê°„ë‹¨í•œ ì„¼ì„œ (ë™, ì„œ, ë‚¨, ë¶)"""
        directions = np.array([[1,0], [-1,0], [0,-1], [0,1]], dtype=np.float32)
        sensors = []
        
        for direction in directions:
            distance = 50.0  # ê¸°ë³¸ ê±°ë¦¬
            
            # ê±´ë¬¼ê³¼ì˜ ìµœë‹¨ ê±°ë¦¬ ê³„ì‚°
            for building in self.buildings:
                bx, by, bw, bh = building
                center = np.array([bx + bw/2, by + bh/2])
                to_building = center - self.position
                
                if np.dot(to_building, direction) > 0:  # í•´ë‹¹ ë°©í–¥ì— ìˆìœ¼ë©´
                    dist = np.linalg.norm(to_building)
                    distance = min(distance, max(dist - 20, 5))
            
            sensors.append(distance / 50.0)  # ì •ê·œí™”
        
        return np.array(sensors, dtype=np.float32)
    
    def _distance_to_target(self):
        """ëª©í‘œê¹Œì§€ì˜ ê±°ë¦¬"""
        return np.linalg.norm(self.waypoints[self.current_waypoint] - self.position)
    
    def step(self, action):
        """ìµœì í™”ëœ ìŠ¤í… í•¨ìˆ˜"""
        self.step_count += 1
        
        # ì•¡ì…˜ ì ìš© (ìŠ¤ì¼€ì¼ë§)
        movement = action * 3.0  # ì›€ì§ì„ í¬ê¸°
        self.velocity = movement * 0.7 + self.velocity * 0.3  # ê´€ì„±
        self.position += self.velocity
        
        # ë³´ìƒ ê³„ì‚°
        reward = self._calculate_optimized_reward()
        
        # ì¢…ë£Œ ì¡°ê±´
        done = self._check_termination()
        
        # ëª©í‘œ ë‹¬ì„± í™•ì¸
        if self._distance_to_target() < 10.0:
            if self.current_waypoint not in self.visited:
                self.visited.append(self.current_waypoint)
                reward += self.config["success_reward"]
            
            self.current_waypoint = min(self.current_waypoint + 1, len(self.waypoints) - 1)
        
        info = {
            "success": len(self.visited) == len(self.waypoints),
            "progress": len(self.visited) / len(self.waypoints),
            "distance": self._distance_to_target()
        }
        
        return self._get_observation(), reward, done, False, info
    
    def _calculate_optimized_reward(self):
        """ìµœì í™”ëœ ê°„ë‹¨í•œ ë³´ìƒ í•¨ìˆ˜"""
        # ëª©í‘œê¹Œì§€ì˜ ê±°ë¦¬ ê¸°ë°˜ ë³´ìƒ
        current_distance = self._distance_to_target()
        progress_reward = (self.last_distance - current_distance) * self.config["progress_reward_scale"]
        self.last_distance = current_distance
        
        # ê¸°ë³¸ í˜ë„í‹°
        reward = progress_reward + self.config["step_penalty"]
        
        # ì¶©ëŒ ê²€ì‚¬ (ê°„ë‹¨í™”)
        if self._is_collision():
            reward += self.config["collision_penalty"]
        
        # ê²½ê³„ ê²€ì‚¬
        if not (10 < self.position[0] < 290 and 10 < self.position[1] < 190):
            reward += self.config["boundary_penalty"]
            self.position = np.clip(self.position, [10, 10], [290, 190])
        
        return reward
    
    def _is_collision(self):
        """ê°„ë‹¨í•œ ì¶©ëŒ ê²€ì‚¬"""
        for building in self.buildings:
            bx, by, bw, bh = building
            if (bx - 5 <= self.position[0] <= bx + bw + 5 and 
                by - 5 <= self.position[1] <= by + bh + 5):
                return True
        return False
    
    def _check_termination(self):
        """ì¢…ë£Œ ì¡°ê±´"""
        return (self.step_count >= self.config["max_steps"] or 
                len(self.visited) == len(self.waypoints))

class OptimizedTrainingCallback(BaseCallback):
    """ìµœì í™”ëœ í•™ìŠµ ì½œë°±"""
    
    def __init__(self, eval_freq=1000, save_freq=5000):
        super().__init__()
        self.eval_freq = eval_freq
        self.save_freq = save_freq
        self.best_mean_reward = -np.inf
    
    def _on_step(self):
        """ìŠ¤í…ë§ˆë‹¤ ì‹¤í–‰"""
        if self.n_calls % self.eval_freq == 0:
            self._evaluate_model()
        
        if self.n_calls % self.save_freq == 0:
            self._save_model()
        
        return True
    
    def _evaluate_model(self):
        """ëª¨ë¸ í‰ê°€"""
        # ê°„ë‹¨í•œ í‰ê°€ ë¡œì§
        recent_rewards = [info.get("episode", {}).get("r", 0) 
                         for info in self.locals.get("infos", [])]
        
        if recent_rewards:
            mean_reward = np.mean(recent_rewards)
            print(f"í‰ê°€ ìŠ¤í… {self.n_calls}: í‰ê·  ë³´ìƒ = {mean_reward:.2f}")
            
            if mean_reward > self.best_mean_reward:
                self.best_mean_reward = mean_reward
                print(f"ìƒˆë¡œìš´ ìµœê³  ì„±ëŠ¥! {mean_reward:.2f}")
    
    def _save_model(self):
        """ëª¨ë¸ ì €ì¥"""
        model_path = f"models/optimized_drone_step_{self.n_calls}.zip"
        self.model.save(model_path)
        print(f"ëª¨ë¸ ì €ì¥: {model_path}")

def create_optimized_env():
    """ìµœì í™”ëœ í™˜ê²½ ìƒì„±"""
    return OptimizedDroneEnv()

def train_optimized_model():
    """ìµœì í™”ëœ ëª¨ë¸ í›ˆë ¨"""
    print("ğŸš€ ìµœì í™”ëœ ë“œë¡  ëª¨ë¸ í›ˆë ¨ ì‹œì‘!")
    
    # ë³‘ë ¬ í™˜ê²½ ì„¤ì •
    num_cpu = min(4, mp.cpu_count())
    env = SubprocVecEnv([create_optimized_env for _ in range(num_cpu)])
    env = VecNormalize(env, norm_obs=True, norm_reward=True)
    
    # ìµœì í™”ëœ ëª¨ë¸ ì„¤ì •
    model = PPO(
        "MlpPolicy",
        env,
        learning_rate=3e-4,
        n_steps=1024,        # ë°°ì¹˜ í¬ê¸° ì¦ê°€
        batch_size=64,       # ë¯¸ë‹ˆë°°ì¹˜ í¬ê¸°
        n_epochs=10,         # ì—í¬í¬ ìˆ˜
        gamma=0.99,          # í• ì¸ ì¸ì
        gae_lambda=0.95,     # GAE ëŒë‹¤
        clip_range=0.2,      # PPO í´ë¦¬í•‘
        ent_coef=0.01,       # ì—”íŠ¸ë¡œí”¼ ê³„ìˆ˜
        vf_coef=0.5,         # ê°’ í•¨ìˆ˜ ê³„ìˆ˜
        max_grad_norm=0.5,   # ê·¸ë˜ë””ì–¸íŠ¸ í´ë¦¬í•‘
        verbose=1,
        device="auto",       # GPU ìë™ ê°ì§€
        tensorboard_log="./tensorboard_logs/"
    )
    
    # ì½œë°± ì„¤ì •
    callback = OptimizedTrainingCallback(eval_freq=2000, save_freq=10000)
    
    # í›ˆë ¨ ì‹œì‘
    print(f"ğŸ”¥ {num_cpu}ê°œ CPU ì½”ì–´ë¡œ ë³‘ë ¬ í›ˆë ¨")
    model.learn(
        total_timesteps=100000,  # ë¹ ë¥¸ í•™ìŠµì„ ìœ„í•´ ì¶•ì†Œ
        callback=callback,
        tb_log_name="optimized_drone_training"
    )
    
    # ìµœì¢… ëª¨ë¸ ì €ì¥
    model.save("models/optimized_drone_final.zip")
    env.save("models/optimized_drone_env.pkl")
    
    print("âœ… ìµœì í™”ëœ ëª¨ë¸ í›ˆë ¨ ì™„ë£Œ!")
    return model, env

def benchmark_training():
    """í›ˆë ¨ ì„±ëŠ¥ ë²¤ì¹˜ë§ˆí¬"""
    import time
    
    print("ğŸ“Š í›ˆë ¨ ì„±ëŠ¥ ë²¤ì¹˜ë§ˆí¬")
    print("=" * 40)
    
    start_time = time.time()
    
    # ê°„ë‹¨í•œ í…ŒìŠ¤íŠ¸ í›ˆë ¨
    env = create_optimized_env()
    model = PPO("MlpPolicy", env, verbose=0)
    model.learn(total_timesteps=1000)
    
    training_time = time.time() - start_time
    
    print(f"âš¡ 1000 ìŠ¤í… í›ˆë ¨ ì‹œê°„: {training_time:.2f}ì´ˆ")
    print(f"ğŸƒ ì´ˆë‹¹ ìŠ¤í… ìˆ˜: {1000/training_time:.1f} steps/sec")
    
    # ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ì²´í¬
    import psutil
    process = psutil.Process()
    memory_mb = process.memory_info().rss / 1024 / 1024
    print(f"ğŸ’¾ ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰: {memory_mb:.1f}MB")

if __name__ == "__main__":
    # ë²¤ì¹˜ë§ˆí¬ ì‹¤í–‰
    benchmark_training()
    
    # ì‹¤ì œ í›ˆë ¨ (ì˜µì…˜)
    # train_optimized_model() 